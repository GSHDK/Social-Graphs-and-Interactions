{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats of the Data and network\n",
    "\n",
    "This section will present some basic information about the dataset and initial preperation to allow for the reader to get a good idea about the data that project evolves around\n",
    "### <span style=\"color:red\">Elaborate</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from IPython.display import Image, clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import bar_chart_race as bcr\n",
    "import random\n",
    "from fa2 import ForceAtlas2\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from ipywidgets import *\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "#import gif\n",
    "\n",
    "\n",
    "#Init stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB 5000\n",
    "### <span style=\"color:red\">WHAT IS THE DATA ETC: WRITE MORE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "RELATIVE_FILE_PATH='../'\n",
    "movies = pd.read_csv(RELATIVE_FILE_PATH+'tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv(RELATIVE_FILE_PATH + 'tmdb_5000_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DF\n",
    "df = movies.merge(right = credits, left_on = 'id', right_on = 'movie_id')\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "\n",
    "\n",
    "#Convert JSON to dict and extract the names of each actor in each movie\n",
    "actors_per_movie = df['cast'].apply(lambda x: ','.join([y['name'] for y in json.loads(x)])).str.split(',', expand = True).fillna('')\n",
    "actors_per_movie = df[['original_title', 'id']].merge(right = actors_per_movie, left_index = True, right_index = True)\n",
    "\n",
    "#Unpivot\n",
    "actors_per_movie = actors_per_movie.melt(id_vars=['original_title', 'id'])\n",
    "\n",
    "#Remove empty rows\n",
    "actors_per_movie = actors_per_movie[actors_per_movie['value'] != '']\n",
    "\n",
    "#Number of movies per actor\n",
    "pt = actors_per_movie.pivot_table(index = 'value', values='id', aggfunc = len).reset_index().sort_values(by='id', ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>value</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45586</td>\n",
       "      <td>Samuel L. Jackson</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jr.</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43599</td>\n",
       "      <td>Robert De Niro</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6819</td>\n",
       "      <td>Bruce Willis</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34677</td>\n",
       "      <td>Matt Damon</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              value  id\n",
       "0  45586  Samuel L. Jackson  67\n",
       "1      2                Jr.  60\n",
       "2  43599     Robert De Niro  57\n",
       "3   6819       Bruce Willis  51\n",
       "4  34677         Matt Damon  48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the original 4803 movies, there are 54199 actors. Of these actors 50394 have starred in less than 5 movies\n"
     ]
    }
   ],
   "source": [
    "print(f\"In the original {len(df)} movies, there are {len(pt)} actors. Of these actors {len(pt[pt['id']<5])} have starred in less than 5 movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decreasing the size of the data\n",
    "\n",
    "We now have the full dataset, but as we can see above there are quite Actors in this network that contribute very little information to the questions we want to answer. Modelling the network with actors as nodes and movies as the links between them we will have alot of low degree nodes potentially disturbing the picture.\n",
    "Many of these low degree actors are most likely extras, but since there are only 4803 movies in the data and as of 2012 the United States has produced roughly 44.000 [movies](https://babelniche.com/2013/06/29/where-are-movies-made/) according to IMDB. \n",
    "\n",
    "Due to this, and also to reduce the complexity we have decided to only look at the top 500 most productive actors and actresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select top N actors by number of movies\n",
    "pt = pt.nlargest(500, 'id')\n",
    "#Top N actors as Nodes\n",
    "nodes = pt['value'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Network itself\n",
    "As mentioned the network will consist of actors as nodes and movies as edges. Of course actors can participate in multiple movies together, and this is also part of our investigation. To handle this we will model the network as a weighted undirected graph where the weight is the number of movies the actors starred together in.\n",
    "\n",
    "The network is also going to contain some other basic stats on nodes and edges:\n",
    "\n",
    "### <span style=\"color:red\">UPDATE IF ASBJÃ˜RN CHANGES</span>\n",
    "\n",
    "**Edges**\n",
    "* Weight\n",
    "\n",
    "**Nodes**\n",
    "* Name of actor/actress\n",
    "* Movie id list\n",
    "* Genre dictionary\n",
    "\n",
    "The reason we only store the id's and not the full data is to reduce redundance and while not decreasing speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network\n",
    "\n",
    "# Extracting genre\n",
    "movie_genre={}\n",
    "for x in movies.iterrows():\n",
    "    movie_genre[x[1]['original_title']]=[y['name'] for y in json.loads(x[1]['genres'])]\n",
    "\n",
    "    \n",
    "#Create edges between actors that have been in the same movie\n",
    "edges = []\n",
    "actor_genre = {}\n",
    "actor_movies_count = {}\n",
    "actor_movies_list = {}\n",
    "edge_movie_lookup = {}\n",
    "for node in nodes:\n",
    "    actor_movies = actors_per_movie[actors_per_movie['value'] == node]['original_title'].tolist()\n",
    "    edges += [(node, x) for x in\\\n",
    "              actors_per_movie[(actors_per_movie['original_title'].isin(actor_movies)) \\\n",
    "                & (actors_per_movie['value'].isin(nodes))]['value'] if node != x]\n",
    "\n",
    "    # Actor\n",
    "    cnt=len(actor_movies)\n",
    "    actor_movies_count[node] = cnt\n",
    "    actor_movies_list[node] = actor_movies\n",
    "    actor_genre[node]=[movie_genre[y] for y in actor_movies]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set attributes\n",
    "\n",
    "ct=Counter(edges)\n",
    "# Add weights\n",
    "for key, value in ct.items():\n",
    "    G.edges[key]['weight'] = value\n",
    "    \n",
    "# Generate list of movies for edges\n",
    "for key, value in edge_movie_lookup.items():\n",
    "    G.edges[key]['movies']=value\n",
    "\n",
    "# Add most frequent genre\n",
    "for actor in G.nodes:\n",
    "    G.nodes[actor]['genre']=Counter([x for y in actor_genre[actor] for x in y])\n",
    "    G.nodes[actor]['top_genre']=G.nodes[actor]['genre'].most_common(1)[0][0]\n",
    "    G.nodes[actor]['top_genre']\n",
    "    G.nodes[actor]['movies_count']=actor_movies_count[actor]\n",
    "    G.nodes[actor]['movies']=actor_movies_list[actor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elisabeth Shue\n",
      "genre: Counter({'Drama': 8, 'Comedy': 7, 'Thriller': 5, 'Romance': 4, 'Science Fiction': 4, 'Horror': 3, 'Adventure': 3, 'Family': 3, 'Action': 2, 'Mystery': 2, 'Crime': 1, 'Music': 1})\n",
      "top_genre: Drama\n",
      "movies_count: 17\n",
      "movies: ['Piranha 3D', 'Molly', 'Hollow Man', 'The Saint', 'Don McKay', 'Leaving Las Vegas', 'Chasing Mavericks', 'House at the End of the Street', 'Back to the Future Part II', 'Dreamer: Inspired By a True Story', 'Hide and Seek', 'Back to the Future Part III', 'The Karate Kid', 'Deconstructing Harry', 'Gracie', 'Hamlet 2', 'Hope Springs']\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at a node in the graph\n",
    "print(actor)\n",
    "for x in G.nodes[actor]:\n",
    "    print (f\"{x}:\",G.nodes[actor][x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKIPEDIA YEAR LOOKUP\n",
    "\n",
    "The second part of the dataset is a lookup of an the year/decades wikipedia page to see if the findings from our analysis corresponds to the sentiment found on the pages. The purpose and hypotesis of this is that we will find a direct correlation between the sentiment of movies and the sorrunding decands data.\n",
    "\n",
    "One assumption here is that will let the production of hollywood reflect the general state of the world. The reason for this is that there is no pattern wikipedia pattern for year/decade lookup only for the us. Scraping multiple sites will take too much time and loose focus of the important stuff. A reason why this assumption might be allright is due to the fact that the earlies movies in the dataset start around 1970's, which means that society is quite globalised already\n",
    "\n",
    "**Cleaning and preprocessing**\n",
    "The data is fetched through the api and is parsed using regex to a raw but filtered state\n",
    "\n",
    "The filtering applied is:\n",
    "* Removing links an references to other pages identified by being within <> or {}\n",
    "* Removing links identified by starting with url=\n",
    "* Removing non alpha numeric charaters\n",
    "* Removing links identified by starting with http\n",
    "\n",
    "This leaves us some partly filtered data still containing stopwords etc. ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The content of the characters' wiki-pages is extracted using the wikipedia API\n",
    "baseurl = 'https://en.wikipedia.org/w/api.php?'\n",
    "action = 'action=query'\n",
    "title = 'titles='\n",
    "content = 'prop=revisions&rvprop=content'\n",
    "dataformat = 'format=json'\n",
    "\n",
    "def look_up_decade(year: int)-> str:\n",
    "    decade_start=int(year/10)*10\n",
    "    query = '%s%s&%s&%s&%s' % (baseurl,action,f'titles={decade_start}s',content,dataformat)\n",
    "    res = json.loads(urllib.request.urlopen(query).read().decode('utf-8'))\n",
    "    pages = res.get('query').get('pages')\n",
    "    if not pages:\n",
    "        raise Exception('No pages found')\n",
    "    data = []\n",
    "    for page in pages.keys():\n",
    "        try:\n",
    "            data.append(res['query']['pages'][page]['revisions'][0]['*'])\n",
    "        except:\n",
    "            print(f\"Failed on pages{page}\")\n",
    "    return data\n",
    "\n",
    "def process_data(d:list, limit=3)->list:\n",
    "    data_string = ''\n",
    "    temp_str = ''\n",
    "    \n",
    "    i=0\n",
    "    for x in d:\n",
    "        if i>=limit:\n",
    "            break\n",
    "        # Remove special chars and data in links\n",
    "        temp_str=re.sub(\"[\\{\\<.*?[\\}\\>]\", \"\", x)\n",
    "        # Remove links\n",
    "        temp_str=re.sub('url=.\\S*','',temp_str)\n",
    "        # Weird chars\n",
    "        temp_str=re.sub('[^a-zA-Z0-9 \\n\\.]', '', temp_str)\n",
    "        # Remaning links\n",
    "        temp_str=re.sub('http.\\S*','',re.sub('[^a-zA-Z0-9 \\n\\.]', '', temp_str))\n",
    "        # Remaning links\n",
    "        temp_str=re.sub('redirect.\\S*','',temp_str)\n",
    "        data_string += temp_str\n",
    "        i+=1\n",
    "    return data_string    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: MOST OF THE FUNCTIONS AND DATAPROCESSING STEPS HAS BEEN MOVED TO A SEPERATE HELPER FILE IN THE OTHER NOTEBOOKS TO IMPORVE READABILITY**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
